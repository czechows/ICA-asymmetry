\documentclass[12pt]{article}

\textheight=25.2cm \topmargin=-2.6cm \hoffset=-2.7cm \textwidth=18cm
\usepackage{amsmath,amsfonts,amssymb,amsthm,dsfont}
\usepackage[utf8]{inputenc}
\usepackage{hyperref}

\def\N{\mathbb{N}}
\def\Z{\mathbb{Z}}
\def\R{\mathbb{R}}
\def\C{\mathcal{C}}

\def\e{\varepsilon}
\def\d{\delta}
\def\w{\omega}
\def\v{\mathrm{V}}
\def\x{\mathrm{x}}
\def\m{\mathrm{m}}

\def\F{\mathcal{F}}
\def\G{\mathcal{G}}
\def\S{\mathcal{S}}
\def\A{\mathcal{A}}

\def\KL{\mathrm{KL}}

\def\for{\mbox{  for }}
\def\mle{\mathrm{mle}}
\def\diag{\mathrm{diag}}
\def\cov{\mathrm{cov}}
\def\card{\mathrm{card}}
\def\aff{\mathrm{aff}}
\def\span{\mathrm{span}}
\def\nor{\mathcal{N}}

\newtheorem{observation}{Observation}[section]
\newtheorem{theorem}{Theorem}[section]
\newtheorem{proposition}{Proposition}[section]
\newtheorem{lemma}{Lemma}[section]
\newtheorem{corollary}{Corollary}[section]

\theoremstyle{definition}
\newtheorem{definition}{Definition}[section]
\newtheorem{remark}{Remark}[section]
\newtheorem{example}{Example}[section]
\newtheorem{problem}{Problem}[section]

\def\w{\omega}
\DeclareMathOperator*{\argmin}{argmin}

\title{ICA for subspaces}
\author{P. Spurek \and J. Tabor}
\date{}

\begin{document}

\maketitle

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Basic tools}


\subsection{Orthogonal projection onto affine subspaces}

Suppose that we have an affine subspace generated over $m \in \R^D,\v \in \R^{D \times d}$, where 
$\v=[v_1,\ldots,v_k]$ (or more precisely its consecutive columns) is the base of linear part of $P$ with $d$ elements, that is
$$
M=m+\span (\v)=m+\{\v r:r \in \R^d\}=\{m+r_1v_1+\ldots+r_dv_d: r_i \in \R\}.
$$
We are interested in the coordinates of the point $x \in \R^D$ after the orthogonal
projection onto $P$ with respect to the base.
This can restated as the search for coordinates $r=(r_1,\ldots,r_d)^T \in \R^d$
such that
$$
r=\argmin_{s \in \R^d} \|x-(m+\v s)\|^2=\argmin_{s \in \R^d} \|x-(m+s_1v_1+\ldots+s_dv_d)\|^2.
$$
The formula can be obtained by the least squares solution to the problem $m+\v r=x$:
$$
r_1v_1+\ldots+r_kv_k=x-m,
$$
which is given by:
$$
r=(\v^T\v)^{-1}\v^T(x-m) \in \R^d.
$$


\subsection{Integration on subspaces}

For the integration over $C^1$ submanifolds of $\R^D$ refer the reader to  \cite{munkres1997analysis, federer2014geometric}. If we are given an a $C^1$ submanifold $M$  of dimension $d$ of $\R^D$, then we have a default restriction of Lebesgue measure to $M$, which we denote by $\lambda_d$ (formally, it is the normalization of $d$-dimensional Haar measure).

In the case we are interested in, when $M$ is an affine subspace, to integrate
a function over $M$ we can take a point $m \in M$ and base $\v$ of the linear part
of $M$, and then
$$
\int_M f(x) d\lambda_d(x)=\det(\v^T\v)^{1/2} \int_{\R^d} f(m+\v r) d\lambda_d(r).
$$

With respect to measure $\lambda_d$ in $\R^D$ we can consider the 
singular densities (that is those defined only on $M$, or equivalently zero 
except for $M$). In the most important case of Gausian densities, if $m \in \R^D$ and $\Sigma$ is a symmetric nonnegative 
matrix with rank $d$, then by $N(m,\Sigma)$ we denote the function with support
in $M=\{m+\Sigma^{1/2}r : r\in \R^D\}$ and the density given by
$$
\nor(m,\Sigma)(x)=\frac{1}{\sqrt{\det^{*}(2\pi \Sigma )}} e^{-\frac{1}{2}(x -m)^T{\Sigma }^\dagger (x -m)} \text{ for } x \in M,
$$
where $\Sigma^\dagger$ is the generalized Moore-Penrose inverse and $\det^*$ is the pseudo-determinant\footnote{That is the product of all nonzero eigenvalues.}.

\subsection{Push-forward of measures}

Since we know how to integrate functions on affine subspaces, let us discuss the
natural method of defining (by push-forward) measures on such subspaces,
for more information see \url{https://en.wikipedia.org/wiki/Pushforward_measure},
\url{http://www.mat.univie.ac.at/~gerald/ftp/book-fa/index.html} (page 256) and
\cite{bogachev2007measure}. We assume as before, that 
$M$ is an affine subspace of $\R^D$ of dimension $d$, and that we fix $m$ (cordinate center) and $\v$ (base of linear part of $M$).
Assume that we are given an affine function
$$
a: \R^d \ni r \to m+\v r \in M \subset \R^D.
$$
Then $m$ and $\v$ introduce a coordinate system on $V$, with center at $m$. 

Suppose that we are given a measure $\mu$ on $\R^d$ with density $f$. Then
we can push-forward (transport) the measure $\mu$ onto $M$ through the map $a$ to obtain the measure by the formula
$$
(a_* \mu)(B)=\mu(a^{-1}B) \text{ for } B \subset \R^D.
$$
By applying the knowledge of integration over submanifolds, we obtain that 
the measure $a_* \mu$ with support in $M$ has the singular density with respect to $\lambda_d$ given by
\begin{equation} \label{eq:dens}
f_{m,\v}(x)= 
\begin{cases}
\frac{1}{\sqrt{\det(\v^T\v)}}
 f(a_{m,\v}^{-1}x) \text{ if } x \in M, \\
 0 \text{ otherwise}.
 \end{cases}
\end{equation}

Roughly speaking the above means that if we have a data-set $W \subset \R^d$ which comes from the density $f$ on $\R^d$, then $a_{m,V}(W)$ is clearly supported in $M$ and comes from the singular density $f_{m,\v}$ given by 
\eqref{eq:dens}.

In the particual case when $W$ comes from the normal density $\nor(m_d,\Sigma_d)$ in $\R^d$, then $a_{m,\v}(W)$ has the singular normal density $\nor(m+\v m_d,\v^T \Sigma_d \v )$ in $\R^D$.


\subsection{Measure of nongaussianity}

We consider the similar idea to the Kullback-Leibler.

\subsection{Construction of densities}

We can define the family of singular densities on affine subspaces of 
dimension $d$, by taking the transport.


In this subsection we describe the basic construction of product measures and densities. Given functions $f_1,f_2$ on $\R^{d_1},\R^{d_2}$ by 
$$
(f_1 \otimes f_2)(x_1,x_2)=f_1(x_1) \cdot f_2(x_2) \for (x_1,x_2) \in \R^{d_1} 
\times \R^{d_2}
$$ 
we denote the tensor
product of $f_1$ and $f_2$. Observe that if $f_1,f_2$ are densities, then so is
$f_1 \otimes f_2$.

If $\F$ is a family of densities on $\R$, then by $\F^{\otimes d}$ ($d$-th tensor power of $\F$) we denote
the family of densities on $\R^d$ given by
$$
\F^{\otimes k}=\{f_1 \otimes \ldots \otimes f_d : f_i \in \F\}.
$$




\subsection{Our case}

We assume that we have a family $\F$ larger then Gaussians on $\R$.

We have
$$
\KL(X,\aff(\S^{\otimes d}),\G^d)=\inf_{m,\v} \KL((v^T\v)^{-1}\v^T(X-m),\S^{\otimes d},\G).
$$
Notation: $x[m,\v]$. By the $i$-th coordinate we denote $x[m,\v]_i$.

Thus 
$$
\KL(X,\aff(\S^{\otimes d}),\G^d)=\inf_{m,\v} \left( \sum_{i=1}^d \mle(X[m,\v]_i,\S)
-\mle(X[m,\v],\G)
\right),
$$
where the minus has the direct formula which can be computed.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{First approach: global estimation}

We search for the split $g=f \otimes \N$, where $g$ is a normal density on $\R^{D-d}$,
and $f$ is $d$-dimensional. More precisely, we fix a family $\F$ of densities on $\R^d$, and seek $m,\v$ which maximize the MLE:
$$
X \sim a_*(f \otimes g) \text{ for } f \in \F, g \in \nor(\R^{D-d}), a=a_{m,\v} \in \aff(\R^D). 
$$

In the case when $\F$ is one dimensional, the above can be written as:
$$
X \sim \det W \cdot f_1(w_1 \circ (x-m)) \cdot \ldots \cdot f_d(w_d \circ (x-m))
\cdot g_{d+1}(w_{d+1} \circ (x-m)) \cdot \ldots \cdot g_D(w_D \circ (x-m))
$$
where $W=[w_1,\ldots,w_D]=??(V^{-1})^T$.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Second approach: projection}

We want to find an index which would have the following characteristics:
\begin{enumerate}
\item the more non-gaussian data the better,
\item for gaussian data the value zero,
\item invariant under affine transformations.
\item ???? $k(f * N) <k(f)$ which implies the minimization?
\end{enumerate}

\begin{theorem}
? Theorem: in the perfect split we obain original split ?
\end{theorem}

\begin{proof}
We have two random variables which are independent, the second Gaussian.
Observe that if the change of coordinates, then sum of independent variables.

We search for minimal entropy (maximal likelihood). Since
[Original Entropy Power Inequality]
$$
e^{2H(X+Y)} \geq e^{2H(X)}+e^{2H(Y)},
$$
and the equality holds only for the gaussians, 
\end{proof}

We propose the possible solution for the ICA. We assume that we are given an affine-invariant family
$\F$ of densities on $\R^D$, which contains normal densities $\G$ (Gaussians). To measure the distance from 
normality, we define an analogue of Kullback-Leibler divergence [sprawdzic znak, jak entropia to odwrotnie?]:
$$
\KL(X,\F,\G)=\mle(X,\F)-\mle(X,\G).
$$

[czy bierzemy znormalizowane - czy sumaryczne?]

Observe that for a fixed data the second element depends only on the covariance of the data. On the other hand, the first component typically has to be optimized by some gradient methods. Since the formula for the previous part is known
$$
\mle(X,\G)=\card X(-\frac{1}{2}\ln |\Sigma_X|-\frac{D}{2}\ln(2\pi e)).
$$

Now consider the situation where we are given a task of finding dimension on possibly smaller space of dimension $d \leq D$.
In this case assume that we are given a family $\F^d$ on $\R^d$, where $d \leq D$ (we do not assume that $\F^d$ is affine invariant, as we obtain it directly from the construction by the fact that we can adapt the base).
To fix an affine space $V$ of dimension $d$ in $\R^D$ we choose its center $m$
and $d$ linearly independent elements $\v=v_1,\ldots,v_d \in \R^D$.

Now the coordinates\footnote{The formula is the direct consequence of the fact that the orthogonal projection is exactly the solution of least squares solution of the equations 
$v \alpha=x-m$, where $\alpha=(\alpha_1,\ldots,\alpha_d)^T$} in the base $\v$ of orthogonal projection of $x \in \R^D$ onto $V$ is given by
\begin{equation} \label{eq:coord}
\lambda_{m,\v}^x=(\v^T\v)^{-1}\v^T(x-m) \in \R^d \text{ and }
x_{m,v}=m+\v\lambda_{m,\v}^x.
\end{equation}
By $\Lambda_{m,\v}=(\lambda^x_{m,v})$ we denote the coordinates of the whole data set.
Now we can project the data to this space, and in those coordinates we can measure
the previously defined Kullback-Leibler generalized divergence:
\begin{equation} \label{eq:KL}
(m,\v) \to \KL(\Lambda_{m,\v},\F^d,\G).
\end{equation}
The minimization of the above function leads to the solution of the ICA
problem on the respective subspace.

We will consider it for the family $\F$ of split Gaussians, however, one can apply any family used in the ICA process.
 
	 It occurs that under weak assumption we can even rank the base vectors of $\v$. To do so suppose that $\S^{\otimes d}$ is given as tensor product 
$\S^{\otimes d}=\S \otimes \cdots \otimes \S$, where $\S$ denotes a family of densities on $\R$
(this is the case of split Gaussians). 
In other words we assume that every element of $F \in \S^d$ can be decomposed in the form 
$$
F(x_1,\ldots,x_d)=f_1(x_1) \cdot \ldots \cdot f_d(x_d)\text{ where }f_i \in \S.
$$
Notation $\aff(S^{\otimes d})$ -- will denote the space of affine. If we are given
a density $f$ on $\R^d$, and an affine map $A:\R^d \ni \lambda \to m+\v \lambda$, then
the degenerate density on the space $V$ with respect to the $d$-dimensional Lebesgue (Haar) measure  $\lambda_d$ is given by
$$
f_V:V \ni x \to \frac{1}{|A|}f(A^{-1}x)
$$
where $|A|$ is the generalization of determinant given by ... 
The formula for the KL is therefore given by
$$
\sum_x \ln f(A^{-1}p_Vx) -\ln N(A^{-1}p_Vx).
$$
Observe that $\Sigma \Lambda_V=(A^{-1}p_V)\Sigma(A^{-1}p_V)^T$.
Consequently, the minus part equals
$$
\card X(-\frac{1}{2}\ln |(A^{-1}p_V)\Sigma(A^{-1}p_V)^T|-\frac{d}{2}\ln(2\pi e)).
$$

PROCEDURE to compute $\KL^d_{m,\v}(X,\S)$:
\begin{itemize}
\item data $X$ and family of one-dimensional densities on $\S$ given,
\item fix $m,\v$,
\item put $\Lambda=(\lambda_{m,\v}^x)_{x \in X} \subset \R^d$,
\item by $\Lambda_i$ we denote the set consisting of $i$-th coordinate of $\Lambda$,
\item compute\footnote{sometimes we need optimization}
$$
\KL(\Lambda,\S^d,\G)=\sum_{i=1}^d \mle(\Lambda_i,\S)-\mle(\Lambda,\G).
$$
\end{itemize}

We put
$$
\KL^d(X,\S)=\inf \KL^d_{m,\v}(X,\S).
$$

\begin{theorem}
a) Independent of affine transformations b) czy mozemy sie zawezic do popdrzestrzeni
\end{theorem}

\begin{problem}
czy jest znany wzor dla mle przy split gaussian?
\end{problem}

\begin{theorem}

\end{theorem}

Now suppose that we have found a base $m,\v$ which minimizes \eqref{eq:KL}.
Denote by $(\alpha)_i$
$i$-th coordinate of $\alpha$, then we can rank the vectors according to the non-gaussianity of the $i$-th coordinate of the projection:
$$
i \to \KL((X_{m,\v})_i,\F^1,\G).
$$
 
We want to introduce a new measure to see if the subspace we found is correct.
The model has to be affine independent. To do so, assume that we are given
data $X=(x_i)$ and the transformed/obtained data $\tilde X=(\tilde x_i)$.
We define the measure between the best affine transformation between data, to do 
so by mean squares we solve the problem
$$
A\tilde x_i+b=x_i.
$$
The mean squarred error is the desired value:
$$
i(X,\tilde X)=\frac{1}{N}\sum_{i=1}^N \|x_i-(A\tilde x_i+b)\|^2.
$$
 
\begin{example}
Take the real-data $X \subset \R^d$, add the next $D-d$ coordinates by some normal density -- we obtain new data set $\tilde X$. Try to find the first $d$ coordinates. 

Measure the value of
$$
i(X,\tilde X).
$$
\end{example}
 
\begin{example}
Take the real-data $X \subset \R^d$, add the next $D-d$ coordinates with zeros.
Next perturb all coordinates by some normal density. Try to find the first $d$ coordinates. Come back by least squares between the original coordinates and the projection.
\end{example}
 
%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Przemek}
%%%%%%%%%%%%%%%%%%%%%%%%%

 The density of the one-dimensional Split Gaussian distribution is given by the formula
$$
SN(x;m,\sigma^2,\tau^2) = \left\{ \begin{array}{ll}
c \cdot \exp[-\frac{1}{2\sigma^2}(x-m)^2], & \textrm{where $x\leq m$}\\
c \cdot \exp[-\frac{1}{2\tau^2\sigma^2}(x-m)^2], & \textrm{where $x>m$}\\
\end{array} \right.
$$
where $c=\sqrt{\frac{2}{\pi}}\sigma^{-1}(1+\tau)^{-1}$. 

A natural generalization of the univariate split normal distribution to the multivariate settings was presented by \cite{villani2006multivariate}.
%\comment{(\cite{john1982three})}
Roughly speaking, authors assume that a vector $\x \in \R^d$ follows the multivariate Split Normal distribution, if its principal components are orthogonal and follow the one-dimensional Split Normal distribution.

\begin{definition}\label{def:SN}
A density of the multivariate Split Normal distribution is given by
$$
 SN_{d}(\x; \m, \sigma,\tau)= \prod_{j=1}^{d} SN(x_j;m_j,\sigma_j^2,\tau_j^2),
$$
where  $\m = [m_1, \ldots, m_d]^T$, $\sigma = [\sigma_{1}^2,\ldots,\sigma_{d}^2]^T$ and $\tau=[\tau_{1}^2,\ldots,\tau_{d}^2]^T$.
%where $W$ is the orthonormal matrix and $\w_{j}$ stand for the $j$-th column of $W$, $\m = (m_1, \ldots, m_d)^T$, $\sigma = (\sigma_{1},\ldots,\sigma_{d})$ and $\tau=(\tau_{1},\ldots,\tau_{d})$.
\end{definition}


In our case we will use density on projection on $d<D$ subspaces. Therefore we need a density $d$-subspace Split Normal distribution.

\begin{definition}\label{def:GSN}
A density of the multivariate $d$-subspace Split Normal distribution is given by
$$
 SN_{d<D}(\x; \m,W, \sigma^2,\tau^2)=  SN_d((W^TW)^{-1}W^T(\x-\m);0,\sigma^2,\tau^2),
$$
where
%%%%%%%%%
$(W^TW)^{-1}W^T(x-m) \in \R^d$
%%%%%%%%%
 $\w_{j} \in \R^D$ is the $j$-th column of non-singular matrix $W = [w_{1},\ldots,w_{d}]$, $\m = [m_1, \ldots, m_D]^T$, $\sigma = [\sigma_{1},\ldots,\sigma_{d}]^T$ and $\tau=[\tau_{1},\ldots,\tau_{d}]^T$.
\end{definition}

Let us recall that the standard Gaussian density in $\R^d$ is defined by 
$$
N(\x;\m,\Sigma)=\frac{1}{(2\pi)^{d/2} \det(\Sigma)^{1/2}} \exp \left(-\tfrac{1}{2} (\x-\m)^T \Sigma^{-1}(\x-\m) \right),
$$
where $\m$ denotes the mean, $\Sigma$ is the covariance matrix.

\begin{definition}\label{def:GSN}
A density of the multivariate $d$-subspace Normal distribution is given by
$$
 N_{d<D}(\x; \m, \Sigma, W)= N((W^TW)^{-1}W^T(\x-\m);0,\Sigma),
$$
where
%%%%%%%%%
$(W^TW)^{-1}W^T(\x-\m) \in \R^d$
%%%%%%%%%
 $\w_{j} \in \R^D$ is the $j$-th column of non-singular matrix $W = [w_{1},\ldots,w_{d}]$, $\m = [m_1, \ldots, m_D]^T$, $\Sigma = \diag(\sigma_{1}^2,\ldots,\sigma_{d}^2)$.
\end{definition}

Our goal is to minimize
$$
\KL(X,\F,\G)=\mle(X,\F)-\mle(X,\G) 
$$
In our language
\begin{equation}
\begin{array}{l}
\KL_{d<D}(X;\m,W,\sigma,\tau,\Sigma) = \\[6pt]
= \sum \limits_{\x \in X} \ln(SN_{d<D}(\x;\m,W,\sigma,\tau)) -
   \sum \limits_{\x \in X} \ln(N_{d<D}(\x;\m,\Sigma,W))
\end{array}
\end{equation}
We known
$$
\sum \limits_{\x \in X} \ln(N_{d<D}(\x;\m,\Sigma,W)) = -\frac{d}{2}\ln(2\pi e)-\frac{1}{2}\ln \det(\Sigma_{W}), 
$$
where 
$$
\Sigma_{W} = \cov( \{ (W^TW)^{-1}W^T(\x-\m) \colon \x \in \R^D\} )
$$


\bibliographystyle{plain}
\bibliography{ref}

\end{document}

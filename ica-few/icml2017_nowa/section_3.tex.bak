
In this section we introduce how our method works in the case of $ICA_{SG}$ which is based on the non-symmetry of the data. 
The method use asymmetrical density distributions -- General Split Gaussian.

The density of the one-dimensional Split Gaussian distribution \cite{villani2006multivariate} is given by the formula
$$
SN(x;m,\sigma^2,\tau^2) = \left\{ \begin{array}{l}
c \cdot \exp[-\frac{1}{2\sigma^2}(x-m)^2], \textrm{$x\leq m$}\\
c \cdot \exp[-\frac{1}{2\tau^2\sigma^2}(x-m)^2], \textrm{$x>m$}\\
\end{array} \right. \!\!\!\!,
$$
where $c=\sqrt{\frac{2}{\pi}}\sigma^{-1}(1+\tau)^{-1}$. 


As we see the split normal distribution arises from merging two opposite halves of two probability density functions of normal distributions in their common mode.
In general the use of the Split Gaussian distribution (even in 1D) allows to fit data with better precision (from the likelihood function point of view). In 1982 John \cite{john1982three} showed that the likelihood function can be expressed in an intensive form, in which the scale parameters $\sigma$ and $\tau$ are a function of the location parameter $m$.
Thanks to this theorem we can maximize the likelihood function numerically with respect to a single parameter $m$ only. The rest of parameters are explicitly given by simple formulas.

A natural generalization of the univariate split normal distribution to the multivariate settings was presented by \cite{spurek2017general}.
Roughly speaking, authors assume that a vector $\x \in \R^d$ follows the multivariate Split Normal distribution, if its principal components are not-orthogonal and follow the one-dimensional Split Normal distribution.

In our case we assume that first $d$ components are model by Split Normal distribution and $D-d$ are simply described by classical Gaussian distributions.


\begin{definition}\label{def:GSN}
A density of the multivariate Split Normal $d$ and Normal $D-d$ distribution is given by
$$
\begin{array}{l}
SN_{d}N_{D-d}(\x; \m,W, \sigma^2,\tau^2)=\\[6pt]
\det(W) \prod_{j=1}^{d} SN(\w_j^T(\x-\m);0,\sigma_j^2,\tau_j^2) \cdot\\[6pt]
\cdot \prod_{j=d+1}^{D} N(\w_j^T(\x-\m);0,\sigma_j^2),
\end{array}
$$
where $\w_{j}$ is the $j$-th column of non-singular matrix $W$, $\m = (m_1, \ldots, m_d)^T$, $\sigma = (\sigma_{1},\ldots,\sigma_{d})$ and $\tau=(\tau_{1},\ldots,\tau_{D-d})$.
\end{definition}

Now we have to maximize the likelihood function with respect to four parameters. In the case of the General Split Normal distribution (contrary to the classical Gaussian one) we do not have explicit formulas and consequently we heave to solve the optimization problem.

The density of the GSN distribution depends on four parameters $\m \in \R^d$, $W \in \M(\R^d)$, $\sigma \in \R^d$, $\tau \in \R^D$. 
We can find them by minimizing the simpler function, which depends on only  $m \in \R^d$ and $W \in \M(\R^d)$. Other parameters are given by explicit formulas.   

\begin{theorem}\label{the:min}
Let $\x_1,\ldots,\x_n$ be given.  
Then the likelihood maximized w.r.t. $\sigma$ and $\tau$ is
\begin{equation}\label{eq:1}
\begin{array}{l}
 \hat{L}(X;\m,W) =   \frac{ 2^{(d-D/2)n} n^{dn/2} }{(\pi e)^{Dn/2}} \cdot \\[1ex]
 \cdot \bigg( \frac{1}{|\det(W)|^{\frac{2}{3}}} \prod\limits_{j=1}^{d} g_{j}(\m,W) \bigg)^{-3n/2} 
\bigg( \prod\limits_{j=d+1}^{D} \frac{(s_1+s_2)}{n} \bigg)^{-n/2},
\end{array}
\end{equation}
where
$$
\begin{array}{c}
{g}_{j}(\m,W) = {s}_{1j}^{1/3} + {s}_{2j}^{1/3},
\\[1ex]
{s}_{1j}= \! \sum\limits_{i \in I_j}[ \w_{j}^T (\x_i-\m)]^2,  {I}_j=\{ i  \colon \w_{j}^T (\x_i-\m) \leq 0 \},
\\[1ex]
{s}_{2j}= \! \sum\limits_{i \in I_j^c}[ \w_{j}^T (\x_i-\m)]^2, {I}_j^c=\{ i \colon  \w_{j}^T (\x_i-\m) > 0 \},
\end{array}
$$
and the maximum likelihood estimators of $\sigma_{j}^2$ and $\tau_{j}$ are
\begin{equation}\label{eq:est}
\begin{array}{l}
\hat \tau_{j}(\m,W)=\left(\frac{s_{2j}}{s_{1j}}\right)^{1/3}, \qquad 1 \leq j \leq d\\[6px]
\hat \sigma_j^2(\m,W) = \left\{ \begin{array}{l}
\tfrac{1}{n} s_{1j}^{2/3} g_{j}(\m,W), \; 1 \leq j \leq d\\
\tfrac{1}{n} (s_{1j}+s_{2j}), \qquad d < j \leq D\\
\end{array} \right. \!\!\!\!.
\end{array}
\end{equation}
\end{theorem}
%\comment{Przemek R.- task 3. sprawdzic dowod Theorem \ref{the:min}}

\begin{proof}
See Section \ref{a1} (Appendix A).
\end{proof}

Thanks to the above theorem, instead of looking for the maximum of the likelihood function, it is enough to obtain the maximum of the simpler function which depends on two parameters $\m \in \R^d$ and $W \in \M(\R^d)$

\begin{equation}\label{equ:ll}
{l}(X;\m,W) = \frac{1}{|\det(W)|^{\frac{2}{3}}} \prod_{j=1}^{d} {g}_{j}(\m,W) \prod_{j=d+1}^{D} (s_{1j} + s_{2j})^{\frac{1}{3}}
\end{equation}
where $w_{j}$ stands for the $j$-th column of matrix $W$. 
Consequently, maximization of likelihood function is equivalent to minimization of  (\ref{equ:ll}), see the following corollary.

\begin{corollary}\label{c2}
Let $X \subset \R^d$, $\m \in \R^d$, $W \in \M(\R^d)$ be given, then
$$
 \argmax_{\m,W} \hat{L}(X;\m,W) =  \argmin_{\m,W} {l}(X;\m,W).
$$
\end{corollary}

One of the possible methods of optimization is the gradient method. Since the minimum of ${l}$ is equal to the minimum of $\ln({l})$, in this subsection we calculate the gradient of $\ln({l})$. 
Before we prove suitable Theorem \ref{ther:grad}, we recall the following lemma.

\begin{lemma}\label{jacobi}
%\comment{Jacek mowi, ze powino być A(t), i napisać co to znaczy adj()}
Let $A = (a_{ij})_{1 \leq i,j \leq d}$ be a differentiable map from real numbers to $d \times d$ matrices then
\begin{equation}
\frac{\partial \det(A)}{\partial a_{ij}} = \mathrm{adj}^T(A)_{ij},
\end{equation}
where $\mathrm{adj}(A)$ stands for the adjugate of $A$, i.e. the transpose of the cofactor matrix.
\end{lemma}
\begin{proof}
By the Laplace expansion $\det A = \sum\limits_{j=1}^{d} (-1)^{i+j} a_{ij} M_{ij}$ where $M_{ij}$ is the minor of the entry in the $i$-th row and $j$-th column. Hence
$$\frac{\partial \det A}{\partial a_{ij}} = (-1)^{i+j} M_{ij} = \mathrm{adj}^T(A)_{ij}.$$
\end{proof}
Now we are ready to calculate gradient of our cost function.

%\begin{theorem}\label{ther:grad}
%Let $X \subset \R^d$, $\m = (\m_1, \ldots, \m_d)^T \in \R^d$, $W = (\w_{ij})_{1 \leq i,j \leq d}$ non-singular be given. 
%Then
%$\nabla_{\m}  \ln {l}(X;\m,W) = \left(  \frac{\partial \ln {l}(X;\m,W)}{\partial \m_1}, \ldots, \frac{\partial \ln {l}(X;\m,W)}{\partial \m_d} \right)^T$,
%where
%$$
%\begin{array}{l}
%\frac{\partial \ln {l}(X;\m,W)}{\partial \m_k} =
%\end{array}
%$$
%Moreover,
%$
%\nabla_{W} \ln {l}(X;\m,W) = \left[ \frac{\partial \ln \tilde{l}(X;\m,W)}{\partial \w_{pk}}  \right]_{1 \leq p,k \leq d},
%$
%where
%$$
%\begin{array}{l}
%\frac{\partial \ln \tilde{l}(X;\m,W)}{\partial \w_{pk}}  = .
%\end{array}
%$$
%and
%$$
%\begin{array}{c}
%%{g}_{j}(\m,W) = {s}_{1j}^{1/3} + {s}_{2j}^{1/3},
%%\\[1ex]
%{s}_{1j}= \! \sum\limits_{i \in I_j}[ \w_{j}^T (\x_i-\m)]^2, {I}_j=\{ i  \colon \w_{j}^T (\x_i-\m) \leq 0 \},
%\\[1ex]
%{s}_{2j}= \! \sum\limits_{i \in I_j^c}[ \w_{j}^T (\x_i-\m)]^2,  {I}_j^c=\{ i  \colon  \w_{j}^T (\x_i-\m) > 0 \}.
%\end{array}
%$$
%\end{theorem}

\begin{theorem}\label{ther:grad}
Let $X \subset \R^d$, $\m = (\m_1, \ldots, \m_d)^T \in \R^d$, $W = (\w_{ij})_{1 \leq i,j \leq d}$ non-singular be given. 
Then
$\nabla_{\m}  \ln {l}(X;\m,W) = \left(  \frac{\partial \ln {l}(X;\m,W)}{\partial \m_1}, \ldots, \frac{\partial \ln {l}(X;\m,W)}{\partial \m_d} \right)^T$,
where
$$
\begin{array}{l}
\frac{\partial \ln {l(X;\m,W)}}{\partial \m_k} =\sum\limits_{j=1}^d \frac{-2}{3({s}_{1j}^{\frac{1}{3}} + {s}_{2j}^{\frac{1}{3}})} \bigg(
\frac{1}{{s}_{1j}^{\frac{2}{3}}} \sum\limits_{i \in I_j} \w_j^T (\x_i - \m)  \w_{jk} + \\[6pt]
\frac{1}{{s}_{2j}^{\frac{2}{3}}} \sum\limits_{i \in I_j^c} \w_j^T (\x_i - \m) \w_{jk}
\bigg)+ %\\[6pt]
\sum\limits_{j=d+1}^D \frac{-2}{3(s_{1j}+s_{2j})} \cdot \\[6pt]\bigg(
 \sum\limits_{i \in I_j} \w_j^T (\x_i - \m)  \w_{jk} + %\\[6pt]
 \sum\limits_{i \in I_j^c} \w_j^T (\x_i - \m) \w_{jk}
\bigg)
.
\end{array}
$$
Moreover,
$
\nabla_{W} \ln {l}(X;\m,W) = \left[ \frac{\partial \ln \tilde{l}(X;\m,W)}{\partial \w_{pk}}  \right]_{1 \leq p,k \leq d},
$
where $\frac{\partial \ln {l(X;\m,W)}}{\partial \w_{pk}} =$
$$
\begin{array}{l}
 -\frac{2}{3} (\w^{-1})^T_{pk} + \frac{2}{3({s}_{1p}^{\frac{1}{3}} +{s}_{2p}^{\frac{1}{3}})} 
 \bigg(
{s}_{1p}^{-\frac{2}{3}} \sum\limits_{ i \in {I}_p} \w^T_p (\x_i - \m) (\x_{ik} - \m_k)\\[6pt]
+ {s}_{2p}^{-\frac{2}{3}} \sum\limits_{ i \in {I}_p^c} \w^T_p (\x_i - \m) (\x_{ik} - \m_k) \bigg)+ \frac{2}{ 3(s_{1j}+s_{2j}) } \bigg( \\[6pt]
\sum\limits_{ i \in {I}_p} \w^T_p (\x_i - \m) (\x_{ik} - \m_k) + \sum\limits_{ i \in {I}_p^c} \w^T_p (\x_i - \m) (\x_{ik} - \m_k) \bigg)
\end{array}
$$
and
$$
\begin{array}{c}
%{g}_{j}(\m,W) = {s}_{1j}^{1/3} + {s}_{2j}^{1/3},
%\\[1ex]
{s}_{1j}= \! \sum\limits_{i \in I_j}[ \w_{j}^T (\x_i-\m)]^2, {I}_j=\{ 1 \leq i \leq n \colon \w_{j}^T (\x_i-\m) \leq 0 \},
\\[1ex]
{s}_{2j}= \! \sum\limits_{i \in I_j^c}[ \w_{j}^T (\x_i-\m)]^2,  {I}_j^c=\{ 1 \leq i \leq n \colon  \w_{j}^T (\x_i-\m) > 0 \}.
\end{array}
$$
\end{theorem}

\begin{proof}
See Section \ref{a2} (Appendix B).
\end{proof}


Thanks to the above Theorem we can use gradient descent, a first-order optimization algorithm. To find a local minimum of the cost function $\ln(l)$ using gradient descent, one takes steps proportional to the negative of the gradient of the function at the current point.




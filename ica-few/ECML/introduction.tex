%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Introduction}
\label{introduction}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

Independent component analysis (ICA) is similar in many aspects to principal component analysis (PCA). In PCA we look for an orthonormal base in which the data components are not
linearly dependent (uncorrelated), while in 
ICA we search for the coordinate system in which the components are independent. More precisely the aim of ICA is to transform the observed data $\X$ into maximally independent components $\S$ with use of an invertible linear transformation $W$, called the {\em transformation matrix}: $$
\S = W^T \X.
$$

Popular ICA methodology does not directly attempt to find components that are independent but rather components that are as non-Gaussian as possible.
This follows from the fact that one of the theoretical foundations of ICA is given by the dual view at the Central Limit Theorem \cite{hyvarinen2000independent}, which states that the distribution of the sum (average or linear combination) of $N$ \textbf{identically distributed?} independent random variables approaches Gaussian as  $N\rightarrow \infty$. Obviously if all source variables are Gaussian, the ICA method will not work. 

Another common approach to ICA based on the maximum likelihood estimation~\cite{pham1997blind} is recently gaining popularity \cite{hyvarinen2004independent,samworth2012independent,ICA2017pattern}.  Then we search for the \textbf{a zamiast the} optimally fitted to data \textbf{moze usunac optimally fitted to data? troche dlugie zdanie} cordinate \textbf{coordinate} system $B$ and marginal
densities $f_i$ such that the data density factors in base $B$ as \textbf{are zamiast as} the product of maringal \textbf{marginal?} densities. To obtain an efficient method and avoid 
overfitting we have to restrict the marginal densities $f_i$ to a class $\F$ of densities which has not too many parameters which can be easily estimated (clearly from obvious reasons this class has to be different from gaussians). \textbf{ostatnie zdanie jest za dlugie i nieczytelne. Wyrzuc which has not too many parameters, zrob ze zdania w nawiasie osobne zdanie, albo wyrzuc,
nie uzywaj clearly i for -- nie from -- obvious reasons obok siebie -- maslo maslane} As $\F$ we typically choose the super-Gaussian logistic density or other heavy tails distributions.

In many applications of ICA we deal with the case when several sensors measure the latents variables and the rest of them record only the noise. This happens when the number of sources is unknown and may be less than the number of sensors (then we are looking for so-called {\em non-square mixing matrix} $W$).
Such a case \textbf{Such a situation aby uniknac powtorzen} is common for example in the identification of brain networks
in functional  magnetic resonance imaging (fMRI) 
\cite{beckmann2012modelling,green2002pca}.
%and then we are looking for so-called non-square mixing matrix.
In practice, most approaches deal with this problem by first applying PCA to the observations prior to classic ICA (PCA+ICA) to meet the assumption of square mixing\textbf{przecinek} and to reduce computational costs \cite{hyvarinen2004independent}. Although numerically effective, this approach may fail as it is not invariant with respect to linear transformation \textbf{transformations}, since PCA will find a ``noise'' component if
it is sufficiently large. \textbf{Zdanie za dlugie i niezrozumiale, rozbij na dwa albo powyrzucaj co nie jest istotne}

%\begin{landscape}
\begin{figure*}[t]
% ensure that we have normalsize text
\normalsize
\begin{center}
  \subfigure[Original images 42049 and 220075 and Gaussian nice. \textbf{noise}] {\label{fig:image_ICA_int_1}
\includegraphics[width=1.4in]{2/2_6_or_1} 
\includegraphics[width=1.4in]{2/2_6_or_2}
\includegraphics[width=1.4in]{2/2_6_or_3}
}
%%%%%%%%%%%%%%%%%%%%
%\subfigure[Sum and subtraction of images.] {\label{fig:image_ICA_int_2}
%\includegraphics[width=1.2in]{2/2_6_sum} 
%\includegraphics[width=1.2in]{2/2_6_div}
%} \\

%%%%%%%%%%%%%%%%%%%%
\subfigure[\ICA.] {\label{fig:image_ICA_int_3}
\includegraphics[width=1.in]{2/2_6_ICA_FEW_II_2} 
}
\subfigure[FastICA.] {\label{fig:image_ICA_int_4}
\includegraphics[width=1.in]{2/2_6_ICA11_2} 
}
\subfigure[ProDenICA.] {\label{fig:image_ICA_int_5}
\includegraphics[width=1.in]{2/2_6_ICA5_2} 
}
\subfigure[NGPP.] {\label{fig:image_ICA_int_9}
\includegraphics[width=1.in]{2/2_6_ICA8_1}
}\\
%%%%%%%%%%%%%%%%%%%%

\subfigure[\ICA.] {\label{fig:image_ICA_int_6}
\includegraphics[width=1.in]{2/2_6_ICA_FEW_II_1}
}
\subfigure[FastICA.] {\label{fig:image_ICA_int_7}
\includegraphics[width=1.in]{2/2_6_ICA11_1}
}
\subfigure[ProDenICA.] {\label{fig:image_ICA_int_8}
\includegraphics[width=1.in]{2/2_6_ICA5_1}
}
\subfigure[NGPP.] {\label{fig:image_ICA_int_10}
\includegraphics[width=1.in]{2/2_6_ICA8_2}
}
\end{center}
\caption{Comparison of images separation by our method (\ICA), with FastICA,  ProDenICA, NGPP. Before mixing by a linear matrix, we added to the first two components given in (a) the third component given by random
normal noise. As we see \ICA{} was able to perfectly recover the two first components.}
\label{fig:image_ICA_int}
\end{figure*}

The aim of this paper is to propose a new\textbf{przecinek} density based approach to deal with \textbf{zamien deal with na tackle} this case which does not have the above
mentioned disadvantage \textbf{wyrzuc od which do konca zdanie, wiadomo o co chodzi} . Our idea is to join the two earlier mentioned approaches to solving ICA - one based on the search for non-gaussian components and the
other based on density estimation - to deal with the case when the number of sources is smaller then that of sensors.  
Observe that the noise typically occurs as a sum of many independent
factors, and consequently thanks to the central limit theorem \textbf{moze usun thanks to the central limit theorem, wszyscy wiedza a za duzo w zdaniu} 
it typically has approximately gaussian distribution \textbf{zamiast approximately gaussian distribution distribution close to gaussian}. This is why
one typically makes the assumption \cite{cover2012elements} that
$$
\text{\em noise components are comming from a gaussian noise.}
$$ 
\textbf{czy to jakas prawda objawiona? wrzuc noise components.. normalnie w zdanie a nie wyroznione jak rownanie}
Following the density approach\textbf{przecinek} to filter them out we fit the first $d$-components from a class of $\F$ of densities which is broader then gaussians, while the rest from the gaussians $\nor$ (the final choice of the value of parameter $d \in \{1,\ldots,D\}$ can be decided by applying either AIC or BIC criterion). Following \cite{ICA2017pattern}\textbf{przecinek} as $\F$ we take the class \SN{} of split-normal densities.

Our experiments show, which is illustrated by Figure \ref{fig:image_ICA_int} \textbf{wyrzuc, wsadz (cf. Figure 1)}, that \ICA{} works as desired\textbf{przecinek} and effectively removes the components which contains \textbf{contain} gaussian noise. However, we cannot objectively conclude that it is better as compared to other state-of-the-art approaches, since the experiment 
was conducted in the setting optimal to our method as we assumed
that the noise was gaussian. \textbf{Musimy sie tak kajac? Przeciez stwierdzamy ze jest lepsza w usuwaniu gaussian noise, wiec ta klaryfikacja ze ogolnie moze nie byc lepsza
to troche na wyrost. Spytaj Jacka ewentualnie}

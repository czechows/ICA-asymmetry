%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Theoretical foundations of \ICA{}}
\label{p}

In this section we present \textbf{the} theoretical foundations of the method.
We begin with the statement of the problem, next we focus our attention on the presentation of \textbf{zamiast tego wszystkiego, next we present} the class of densities we discuss \textbf{used zamiast we discuss}. Last \textbf{przecinek} we show \textbf{compute zamiast show} the gradient of the method \textbf{przecinek} which is needed in the optimization procedure.


\subsection{Statement of the problem}

Since this general \textbf{the zamiast this general} idea of the search for ICA \textbf{for independent components zamiast for ICA} with the use of
maximum likelihood is essential in our further considerations, for the convenience of the reader we first describe it briefly.
Assume that the random vector $\X$ in $\R^D$ has the density function $F(\x)$.
Suppose that the components of $\X$ are not independent, but that
we know (or suspect) that \textbf{moze usunac we know or suspect that?} there is a basis $B$ (we put $W^T=B^{-1}$) such that in that base the
components of $\X$ become independent. Observe \textbf{przecinek} that where \textbf{then zamiast where} $\w_i^T\x$ is the $i$-th coefficient of $\x$ in the basis $B$ ($\w_i$ denotes the $i$-th column of $W$), and therefore there exist densities 
$f_1,\ldots,f_D$ such that
\begin{equation} \label{eq:gen}
F(\x)=\det(W) \cdot f_1(\w_1^T\x) \cdot \ldots \cdot f_d(\w_d^T\x).
\end{equation}
Given $W$ and densities $(f_i)_{i=1}^D$ we introduce notation to represent
RHS \textbf{we denote the right-hand side} of the above equation \textbf{as follows}:
$$
F_W(f_1,\ldots,f_D)(\x)=\det(W) \cdot f_1(\w_1^T\x) \cdot \ldots \cdot f_d(\w_d^T\x).
$$
Thus we may \textbf{Let us now zamiast Thus we may} state the density based formulation of ICA in the case we have only a sample
$X$ from random vector $\X$.

\medskip

\noindent \textbf{THE?} GENERAL ICA PROBLEM (\textbf{the} maximum likelihood formulation). \\{\em Find densities $f_i$ and matrix $W$, so that $F$ given by
\eqref{eq:gen} optimally fits the data $X=(\x_i)$ with respect to the likelihood, that is that the value
$$
\sum_i \log F_W(f_1,\ldots,f_D)(\x_i)
$$
is maximized.
}

\medskip

Since the search over the space of all densities is not feasible, and could lead to overfitting, we naturally have to reduce to a subclass of all densities on $\R$ parametrized by a finite amount of parameters. Clearly, since 
ICA does not work if the data are gaussian, we have to choose a family $\F$ of densities which is distant from Gaussian ones. 

\medskip

\noindent ICA FOR DENSITY CLASS $\F$.\\ {\em Find \textbf{a} matrix $W$ and
densities
$
f_1,\ldots,f_D \in \F, 
$
such that the value of 
$$
\sum_i \log F_W(f_1,\ldots,f_D)(\x_i)
$$
is maximized.
}

\medskip

Similarly to \cite{ICA2017pattern} as a class \textbf{usun a class} $\F$ we are going to
take the class of split-gaussians, as 
as it is easy to deal with (small number of parameters) and is resistant to outliers\footnote{The reason is that split gaussians \textbf{moze split gaussian distributions}, instead at fitting the distribution with respect to heavy tails, fits the asymmetry of the data. \textbf{troche to zdanie ciezkie. Moze po prostu split gaussians are fitting well to asymmetrical data?}}.

As mentioned in the introduction, we assume that components which we would like to filter-out, \textbf{tu bez przecinka} are coming from a gaussian noise, and the \textbf{then zamiast the} aim it to fit the first $d$-components from a larger class of densities, while the rest from the gaussians $\nor$.\textbf{Poprzednie zdanie jest troche niezrozumiale, przepisz prosze (moze rozbij na dwa).} Thus our final problem can be stated as follows.

\medskip

\noindent ICA FOR DENSITY CLASS $\F$ WITH $d$ SOURCES. \\{\em Find \textbf{a} matrix $W$, densities 
$
f_1,\ldots,f_d \in \F \text{ and normal densities } f_{d+1},\ldots,f_D \in \nor,
$
so that the value of 
$$
\sum_i \ln F_W(f_1,\ldots,f_D)(\x_i)
$$
is maximized.
}

\medskip

Observe that the solution to the above problem is linearly invariant, that is if
$W$ is optimal for $X$ an $A$ is linear, then $W_A$ is optimal for $AX$,
where $W_A=(A^{-1})^TW$.

The continuous version of the condition we maximize in the case we know the density $f$
of the random variable $\X$ limits to
$$
\begin{array}{l}
\int \ln F_W(f_1,\ldots,f_D)(\x)f(\x) d\x %\\[1ex]
=-H(f,F_W(f_1,\ldots,f_D)),
\end{array}
$$
where the cross entropy $H(f,g)$ is given by the sum of entropy $H(f)$
and Kullback-Leibler divergence $D_{KL}(f,g)$. Thus the continuous version of the ICA problem
with $d$ sources reduces to the minimization of 
$$
D_{KL}(f,F_W(f_1,\ldots,f_D))
$$
over all matrices $W$ and densities $f_1,\ldots,f_D \in \F$. Since for fixed $f$ Kullback-Leibler divergence is minimized for $g=f$, we arrive at the following result, which says that in the ideal case 
by the discussed approach we restore the unmixing matrix if it exists.

\begin{theorem}
  Let $F$ be a density such that there exist \textbf{a} matrix $\overline W$ and densities
$$
\hat f_1,\ldots,\hat f_d \in \F \text{ and } \hat f_{d+1},\ldots,\hat f_D \in \nor
$$
such that 
$$
F=F_{\overline W}(\hat f_1,\ldots,\hat f_D).
$$
Then
$$
\begin{array}{l}
\overline W,\hat f_1,\ldots,\hat f_D %\\[1ex]
\begin{array}{l}
=\argmin \{F_{W}(f_1,\ldots,f_D): %\\[1ex] 
%\phantom{=\argmin \{}
W, f_1,\ldots,\bar f_d \in \F,f_{d+1},\ldots,f_D \in \nor
\}.
\end{array}
\end{array}
$$
\end{theorem}

\subsection{Split normal distribution}


In this section we discuss the class $\F$ we will use in our final algorithm of \ICA{}. The density of \SN{}, the one-dimensional split normal distribution \cite{villani2006multivariate}, is given by the formula
$$
\SN(x;m,\sigma^2,\tau^2) = \left\{ \begin{array}{l}
c \cdot \exp[-\frac{1}{2\sigma^2}(x-m)^2], \textrm{$x\leq m$}\\
c \cdot \exp[-\frac{1}{2\tau^2\sigma^2}(x-m)^2], \textrm{$x>m$}\\
\end{array} \right. \!\!\!\!,
$$
where $c=\sqrt{\frac{2}{\pi}}\sigma^{-1}(1+\tau)^{-1}$. 


As we see \textbf{przecinek} the split normal distribution comes from merging two opposite halves of two normal distributions in their common mode. The main advantage of \textbf{in using zamiast of} split normal distributions over normal one \textbf{moze over regular ones?} is that it \textbf{they zamiast it} allows \textbf{allow zamiast allows} data asymmetry. In 1982 John \cite{john1982three} showed \textbf{Moze It was shown by John.. i bez pisania roku} that the likelihood function can be expressed in a  form in which the scale parameters $\sigma$ and $\tau$ are an explicit function of the location parameter $m$.
In the case when $\F=\SN$
the density class considered in the previous subsection is given in the
explicit form by the following observation.

\begin{observation}\label{def:GSN}
  \textbf{Czemu Observation 31? Przenumeruj to jakos} A density of the multivariate split normal $d$ and normal $D-d$ distribution is given by
$$
\begin{array}{l}
\SN_{d}\nor_{D-d}(\x; \m,W, \sigma^2,\tau^2)=\\[6pt]
\det(W) \prod \limits_{j=1}^{d} \SN(\w_j^T(\x-\m);0,\sigma_j^2,\tau_j^2)%\cdot\\[1ex]
\cdot \prod \limits_{j=d+1}^{D} \nor(\w_j^T(\x-\m);0,\sigma_j^2),
\end{array}
$$
where $\w_{j}$ is the $j$-th column of non-singular matrix $W$, $\m = (m_1, \ldots, m_d)^T$, $\sigma = (\sigma_{1},\ldots,\sigma_{d})$ and $\tau=(\tau_{1},\ldots,\tau_{D-d})$.
\end{observation}

Observe that the above density probability function has mode in $\m$.
As a consequence of result of John \cite{john1982three} we can maximize the likelihood of the above function on data $X$ with respect to $\sigma$ and $\tau$.

\begin{theorem}\label{the:min}
Let $\x_1,\ldots,\x_n$ be given, and let $\m \in \R^D$ and matrix $W$
be fixed.  
Then the likelihood maximized w.r.t. $\sigma$ and $\tau$ is
\begin{equation}\label{eq:1}
\begin{array}{l}
 \hat{L}(X;\m,W) =   \frac{ 2^{(d-D/2)n} n^{dn/2} }{(\pi e)^{Dn/2}} %\cdot \\[1ex]
 \bigg( \frac{1}{|\det(W)|^{\frac{2}{3}}} \prod\limits_{j=1}^{d} g_{j}(\m,W) \bigg)^{-3n/2} 
\bigg( \prod\limits_{j=d+1}^{D} \frac{(s_1+s_2)}{n} \bigg)^{-n/2},
\end{array}
\end{equation}
where
$$
\begin{array}{c}
{g}_{j}(\m,W) = {s}_{1j}^{1/3} + {s}_{2j}^{1/3},
\\[1ex]
{s}_{1j}= \! \sum\limits_{i \in I_j}[ \w_{j}^T (\x_i-\m)]^2,  {I}_j=\{ i  \colon \w_{j}^T (\x_i-\m) \leq 0 \},
\\[1ex]
{s}_{2j}= \! \sum\limits_{i \in I_j^c}[ \w_{j}^T (\x_i-\m)]^2, {I}_j^c=\{ i \colon  \w_{j}^T (\x_i-\m) > 0 \},
\end{array}
$$
and the maximum likelihood estimators of $\sigma_{j}^2$ and $\tau_{j}$ are
\begin{equation}\label{eq:est}
\begin{array}{l}
\hat \tau_{j}(\m,W)=\left(\frac{s_{2j}}{s_{1j}}\right)^{1/3}, \qquad 1 \leq j \leq d\\[6px]
\hat \sigma_j^2(\m,W) = \left\{ \begin{array}{l}
\tfrac{1}{n} s_{1j}^{2/3} g_{j}(\m,W), \; 1 \leq j \leq d\\
\tfrac{1}{n} (s_{1j}+s_{2j}), \qquad d < j \leq D\\
\end{array} \right. \!\!\!\!.
\end{array}
\end{equation}
\end{theorem}
%\comment{Przemek R.- task 3. sprawdzic dowod Theorem \ref{the:min}}

\begin{proof}
See Section \ref{a1} (Appendix A).
\end{proof}

Thanks to the above theorem we can reduce the search for the maximum of the log-likelihood function for two parameters $\m \in \R^d$ and $W \in \M(\R^d)$.

\begin{equation}\label{equ:ll}
{l}(X;\m,W) = \frac{1}{|\det(W)|^{\frac{2}{3}}} \prod_{j=1}^{d} {g}_{j}(\m,W) \prod_{j=d+1}^{D} (s_{1j} + s_{2j})^{\frac{1}{3}}
\end{equation}
where $w_{j}$ stands for the $j$-th column of matrix $W$. 
Consequently, maximization of likelihood function is equivalent to minimization of  $\ln l$.

\begin{corollary}\label{c2}
Let $X \subset \R^d$, $\m \in \R^d$, $W \in \M(\R^d)$ be given, then
$$
\argmax_{\m,W} \hat{L}(X;\m,W) =  \argmin_{\m,W} \ln {l}(X;\m,W).
$$
\end{corollary}

%\subsection{Optimization}

To minimize $\ln l$ with the use classical gradient descent method \textbf{moze standard gradient methods? We wtyczce uzywamy BFGSa na przyklad to jakas wariacja gradient descentu},  we need
the formula for $\nabla \ln l$ (\textbf{the} gradient of the cost function). 

%Now we are ready to calculate gradient of our cost function.

%\begin{theorem}\label{ther:grad}
%Let $X \subset \R^d$, $\m = (\m_1, \ldots, \m_d)^T \in \R^d$, $W = (\w_{ij})_{1 \leq i,j \leq d}$ non-singular be given. 
%Then
%$\nabla_{\m}  \ln {l}(X;\m,W) = \left(  \frac{\partial \ln {l}(X;\m,W)}{\partial \m_1}, \ldots, \frac{\partial \ln {l}(X;\m,W)}{\partial \m_d} \right)^T$,
%where
%$$
%\begin{array}{l}
%\frac{\partial \ln {l}(X;\m,W)}{\partial \m_k} =
%\end{array}
%$$
%Moreover,
%$
%\nabla_{W} \ln {l}(X;\m,W) = \left[ \frac{\partial \ln \tilde{l}(X;\m,W)}{\partial \w_{pk}}  \right]_{1 \leq p,k \leq d},
%$
%where
%$$
%\begin{array}{l}
%\frac{\partial \ln \tilde{l}(X;\m,W)}{\partial \w_{pk}}  = .
%\end{array}
%$$
%and
%$$
%\begin{array}{c}
%%{g}_{j}(\m,W) = {s}_{1j}^{1/3} + {s}_{2j}^{1/3},
%%\\[1ex]
%{s}_{1j}= \! \sum\limits_{i \in I_j}[ \w_{j}^T (\x_i-\m)]^2, {I}_j=\{ i  \colon \w_{j}^T (\x_i-\m) \leq 0 \},
%\\[1ex]
%{s}_{2j}= \! \sum\limits_{i \in I_j^c}[ \w_{j}^T (\x_i-\m)]^2,  {I}_j^c=\{ i  \colon  \w_{j}^T (\x_i-\m) > 0 \}.
%\end{array}
%$$
%\end{theorem}

\begin{theorem}\label{ther:grad}
Let $X \subset \R^d$, $\m = (\m_1, \ldots, \m_d)^T \in \R^d$, $W = (\w_{ij})_{1 \leq i,j \leq d}$ non-singular be given. 
Then
$\nabla_{\m}  \ln {l}(X;\m,W) = \left(  \frac{\partial \ln {l}(X;\m,W)}{\partial \m_1}, \ldots, \frac{\partial \ln {l}(X;\m,W)}{\partial \m_d} \right)^T$,
where
$$
\begin{array}{l}
\frac{\partial \ln {l(X;\m,W)}}{\partial \m_k} =\sum\limits_{j=1}^d \frac{-2}{3({s}_{1j}^{\frac{1}{3}} + {s}_{2j}^{\frac{1}{3}})} \bigg(
\frac{1}{{s}_{1j}^{\frac{2}{3}}} \sum\limits_{i \in I_j} \w_j^T (\x_i - \m)  \w_{jk} +% \\[6pt]
\frac{1}{{s}_{2j}^{\frac{2}{3}}} \sum\limits_{i \in I_j^c} \w_j^T (\x_i - \m) \w_{jk}
\bigg)+ \\[6pt]
\sum\limits_{j=d+1}^D \frac{-2}{3(s_{1j}+s_{2j})} \cdot %\\[6pt]
\bigg(
 \sum\limits_{i \in I_j} \w_j^T (\x_i - \m)  \w_{jk} + %\\[6pt]
 \sum\limits_{i \in I_j^c} \w_j^T (\x_i - \m) \w_{jk}
\bigg).
\end{array}
$$
Moreover,
$
\nabla_{W} \ln {l}(X;\m,W) = \left[ \frac{\partial \ln \tilde{l}(X;\m,W)}{\partial \w_{pk}}  \right]_{1 \leq p,k \leq d},
$
where
$$
\begin{array}{l}
\frac{\partial \ln {l(X;\m,W)}}{\partial \w_{pk}} = -\frac{2}{3} (\w^{-1})^T_{pk} +\\[6pt]
 \frac{2}{3({s}_{1p}^{\frac{1}{3}} +{s}_{2p}^{\frac{1}{3}})} 
 \bigg(
{s}_{1p}^{-\frac{2}{3}} \sum\limits_{ i \in {I}_p} \w^T_p (\x_i - \m) (\x_{ik} - \m_k)
+ {s}_{2p}^{-\frac{2}{3}} \sum\limits_{ i \in {I}_p^c} \w^T_p (\x_i - \m) (\x_{ik} - \m_k) \bigg)+ \\[6pt]
\frac{2}{ 3(s_{1p}+s_{2p}) } \bigg( 
\sum\limits_{ i \in {I}_p} \w^T_p (\x_i - \m) (\x_{ik} - \m_k) + \sum\limits_{ i \in {I}_p^c} \w^T_p (\x_i - \m) (\x_{ik} - \m_k) \bigg),
\end{array}
$$
and
$$
\begin{array}{c}
%{g}_{j}(\m,W) = {s}_{1j}^{1/3} + {s}_{2j}^{1/3},
%\\[1ex]
{s}_{1j}= \! \sum\limits_{i \in I_j}[ \w_{j}^T (\x_i-\m)]^2, {I}_j=\{ 1 \leq i \leq n \colon \w_{j}^T (\x_i-\m) \leq 0 \},
\\[1ex]
{s}_{2j}= \! \sum\limits_{i \in I_j^c}[ \w_{j}^T (\x_i-\m)]^2,  {I}_j^c=\{ 1 \leq i \leq n \colon  \w_{j}^T (\x_i-\m) > 0 \}.
\end{array}
$$
\end{theorem}

\begin{proof}
See Section \ref{a2} (Appendix B).
\end{proof}


Thanks to the above Theorem we are able to use in our experiments the gradient descent for finding the minimum of our cost function.



\subsection{Orthogonal projection onto affine subspaces}

Suppose that we have an affine subspace generated over $m \in \R^D,\v \in \R^{D \times d}$, where 
$\v=[v_1,\ldots,v_k]$ (or more precisely its consecutive columns) is the base of linear part of $P$ with $d$ elements, that is
$$
M=m+\span (\v)=m+\{\v r:r \in \R^d\}=\{m+r_1v_1+\ldots+r_dv_d: r_i \in \R\}.
$$
We are interested in the coordinates of the point $x \in \R^D$ after the orthogonal
projection onto $P$ with respect to the base.
This can restated as the search for coordinates $r=(r_1,\ldots,r_d)^T \in \R^d$
such that
$$
r=\argmin_{s \in \R^d} \|x-(m+\v s)\|^2=\argmin_{s \in \R^d} \|x-(m+s_1v_1+\ldots+s_dv_d)\|^2.
$$
The formula can be obtained by the least squares solution to the problem $m+\v r=x$:
$$
r_1v_1+\ldots+r_kv_k=x-m,
$$
which is given by:
$$
r=(\v^T\v)^{-1}\v^T(x-m) \in \R^d.
$$

\subsection{Push-forward of measures}

Since we know how to integrate functions on affine subspaces, let us discuss the
natural method of defining (by push-forward) measures on such subspaces,
for more information see \url{https://en.wikipedia.org/wiki/Pushforward_measure},
\url{http://www.mat.univie.ac.at/~gerald/ftp/book-fa/index.html} (page 256) and
\cite{bogachev2007measure}. We assume as before, that 
$M$ is an affine subspace of $\R^D$ of dimension $d$, and that we fix $m$ (cordinate center) and $\v$ (base of linear part of $M$).
Assume that we are given an affine function
$$
a: \R^d \ni r \to m+\v r \in M \subset \R^D.
$$
Then $m$ and $\v$ introduce a coordinate system on $V$, with center at $m$. 

Suppose that we are given a measure $\mu$ on $\R^d$ with density $f$. Then
we can push-forward (transport) the measure $\mu$ onto $M$ through the map $a$ to obtain the measure by the formula
$$
(a_* \mu)(B)=\mu(a^{-1}B) \text{ for } B \subset \R^D.
$$
By applying the knowledge of integration over submanifolds, we obtain that 
the measure $a_* \mu$ with support in $M$ has the singular density with respect to $\lambda_d$ given by
\begin{equation} \label{eq:dens}
f_{m,\v}(x)= 
\begin{cases}
\frac{1}{\sqrt{\det(\v^T\v)}}
 f(a_{m,\v}^{-1}x) \text{ if } x \in M, \\
 0 \text{ otherwise}.
 \end{cases}
\end{equation}

Roughly speaking the above means that if we have a data-set $W \subset \R^d$ which comes from the density $f$ on $\R^d$, then $a_{m,V}(W)$ is clearly supported in $M$ and comes from the singular density $f_{m,\v}$ given by 
\eqref{eq:dens}.

In the particual case when $W$ comes from the normal density $\nor(m_d,\Sigma_d)$ in $\R^d$, then $a_{m,\v}(W)$ has the singular normal density $\nor(m+\v m_d,\v^T \Sigma_d \v )$ in $\R^D$.


\subsection{Integration on subspaces}

For the integration over $C^1$ submanifolds of $\R^D$ refer the reader to  \cite{munkres1997analysis, federer2014geometric}. If we are given an a $C^1$ submanifold $M$  of dimension $d$ of $\R^D$, then we have a default restriction of Lebesgue measure to $M$, which we denote by $\lambda_d$ (formally, it is the normalization of $d$-dimensional Haar measure).

In the case we are interested in, when $M$ is an affine subspace, to integrate
a function over $M$ we can take a point $m \in M$ and base $\v$ of the linear part
of $M$, and then
$$
\int_M f(x) d\lambda_d(x)=\det(\v^T\v)^{1/2} \int_{\R^d} f(m+\v r) d\lambda_d(r).
$$

With respect to measure $\lambda_d$ in $\R^D$ we can consider the 
singular densities (that is those defined only on $M$, or equivalently zero 
except for $M$). In the most important case of Gausian densities, if $m \in \R^D$ and $\Sigma$ is a symmetric nonnegative 
matrix with rank $d$, then by $N(m,\Sigma)$ we denote the function with support
in $M=\{m+\Sigma^{1/2}r : r\in \R^D\}$ and the density given by
$$
\nor(m,\Sigma)(x)=\frac{1}{\sqrt{\det^{*}(2\pi \Sigma )}} e^{-\frac{1}{2}(x -m)^T{\Sigma }^\dagger (x -m)} \text{ for } x \in M,
$$
where $\Sigma^\dagger$ is the generalized Moore-Penrose inverse and $\det^*$ is the pseudo-determinant\footnote{That is the product of all nonzero eigenvalues.}.

\subsection{Second approach: projection}

We want to find an index which would have the following characteristics:
\begin{enumerate}
\item the more non-gaussian data the better,
\item for gaussian data the value zero,
\item invariant under affine transformations.
\item ???? $k(f * N) <k(f)$ which implies the minimization?
\end{enumerate}

\begin{theorem}
? Theorem: in the perfect split we obain original split ?
\end{theorem}

\begin{proof}
We have two random variables which are independent, the second Gaussian.
Observe that if the change of coordinates, then sum of independent variables.

We search for minimal entropy (maximal likelihood). Since
[Original Entropy Power Inequality]
$$
e^{2H(X+Y)} \geq e^{2H(X)}+e^{2H(Y)},
$$
and the equality holds only for the gaussians, 
\end{proof}

We propose the possible solution for the ICA. We assume that we are given an affine-invariant family
$\F$ of densities on $\R^D$, which contains normal densities $\G$ (Gaussians). To measure the distance from 
normality, we define an analogue of Kullback-Leibler divergence [sprawdzic znak, jak entropia to odwrotnie?]:
$$
\KL(X,\F,\G)=\mle(X,\F)-\mle(X,\G).
$$

[czy bierzemy znormalizowane - czy sumaryczne?]

Observe that for a fixed data the second element depends only on the covariance of the data. On the other hand, the first component typically has to be optimized by some gradient methods. Since the formula for the previous part is known
$$
\mle(X,\G)=\card X(-\frac{1}{2}\ln |\Sigma_X|-\frac{D}{2}\ln(2\pi e)).
$$

Now consider the situation where we are given a task of finding dimension on possibly smaller space of dimension $d \leq D$.
In this case assume that we are given a family $\F^d$ on $\R^d$, where $d \leq D$ (we do not assume that $\F^d$ is affine invariant, as we obtain it directly from the construction by the fact that we can adapt the base).
To fix an affine space $V$ of dimension $d$ in $\R^D$ we choose its center $m$
and $d$ linearly independent elements $\v=v_1,\ldots,v_d \in \R^D$.

Now the coordinates\footnote{The formula is the direct consequence of the fact that the orthogonal projection is exactly the solution of least squares solution of the equations 
$v \alpha=x-m$, where $\alpha=(\alpha_1,\ldots,\alpha_d)^T$} in the base $\v$ of orthogonal projection of $x \in \R^D$ onto $V$ is given by
\begin{equation} \label{eq:coord}
\lambda_{m,\v}^x=(\v^T\v)^{-1}\v^T(x-m) \in \R^d \text{ and }
x_{m,v}=m+\v\lambda_{m,\v}^x.
\end{equation}
By $\Lambda_{m,\v}=(\lambda^x_{m,v})$ we denote the coordinates of the whole data set.
Now we can project the data to this space, and in those coordinates we can measure
the previously defined Kullback-Leibler generalized divergence:
\begin{equation} \label{eq:KL}
(m,\v) \to \KL(\Lambda_{m,\v},\F^d,\G).
\end{equation}
The minimization of the above function leads to the solution of the ICA
problem on the respective subspace.

We will consider it for the family $\F$ of split Gaussians, however, one can apply any family used in the ICA process.
 
	 It occurs that under weak assumption we can even rank the base vectors of $\v$. To do so suppose that $\S^{\otimes d}$ is given as tensor product 
$\S^{\otimes d}=\S \otimes \cdots \otimes \S$, where $\S$ denotes a family of densities on $\R$
(this is the case of split Gaussians). 
In other words we assume that every element of $F \in \S^d$ can be decomposed in the form 
$$
F(x_1,\ldots,x_d)=f_1(x_1) \cdot \ldots \cdot f_d(x_d)\text{ where }f_i \in \S.
$$
Notation $\aff(S^{\otimes d})$ -- will denote the space of affine. If we are given
a density $f$ on $\R^d$, and an affine map $A:\R^d \ni \lambda \to m+\v \lambda$, then
the degenerate density on the space $V$ with respect to the $d$-dimensional Lebesgue (Haar) measure  $\lambda_d$ is given by
$$
f_V:V \ni x \to \frac{1}{|A|}f(A^{-1}x)
$$
where $|A|$ is the generalization of determinant given by ... 
The formula for the KL is therefore given by
$$
\sum_x \ln f(A^{-1}p_Vx) -\ln N(A^{-1}p_Vx).
$$
Observe that $\Sigma \Lambda_V=(A^{-1}p_V)\Sigma(A^{-1}p_V)^T$.
Consequently, the minus part equals
$$
\card X(-\frac{1}{2}\ln |(A^{-1}p_V)\Sigma(A^{-1}p_V)^T|-\frac{d}{2}\ln(2\pi e)).
$$

PROCEDURE to compute $\KL^d_{m,\v}(X,\S)$:
\begin{itemize}
\item data $X$ and family of one-dimensional densities on $\S$ given,
\item fix $m,\v$,
\item put $\Lambda=(\lambda_{m,\v}^x)_{x \in X} \subset \R^d$,
\item by $\Lambda_i$ we denote the set consisting of $i$-th coordinate of $\Lambda$,
\item compute\footnote{sometimes we need optimization}
$$
\KL(\Lambda,\S^d,\G)=\sum_{i=1}^d \mle(\Lambda_i,\S)-\mle(\Lambda,\G).
$$
\end{itemize}

We put
$$
\KL^d(X,\S)=\inf \KL^d_{m,\v}(X,\S).
$$

\begin{theorem}
a) Independent of affine transformations b) czy mozemy sie zawezic do popdrzestrzeni
\end{theorem}

\begin{problem}
czy jest znany wzor dla mle przy split gaussian?
\end{problem}

\begin{theorem}

\end{theorem}

Now suppose that we have found a base $m,\v$ which minimizes \eqref{eq:KL}.
Denote by $(\alpha)_i$
$i$-th coordinate of $\alpha$, then we can rank the vectors according to the non-gaussianity of the $i$-th coordinate of the projection:
$$
i \to \KL((X_{m,\v})_i,\F^1,\G).
$$
 
We want to introduce a new measure to see if the subspace we found is correct.
The model has to be affine independent. To do so, assume that we are given
data $X=(x_i)$ and the transformed/obtained data $\tilde X=(\tilde x_i)$.
We define the measure between the best affine transformation between data, to do 
so by mean squares we solve the problem
$$
A\tilde x_i+b=x_i.
$$
The mean squarred error is the desired value:
$$
i(X,\tilde X)=\frac{1}{N}\sum_{i=1}^N \|x_i-(A\tilde x_i+b)\|^2.
$$
 
\begin{example}
Take the real-data $X \subset \R^d$, add the next $D-d$ coordinates by some normal density -- we obtain new data set $\tilde X$. Try to find the first $d$ coordinates. 

Measure the value of
$$
i(X,\tilde X).
$$
\end{example}
 
\begin{example}
Take the real-data $X \subset \R^d$, add the next $D-d$ coordinates with zeros.
Next perturb all coordinates by some normal density. Try to find the first $d$ coordinates. Come back by least squares between the original coordinates and the projection.
\end{example}


 The density of the one-dimensional Split Gaussian distribution is given by the formula
$$
SN(x;m,\sigma^2,\tau^2) = \left\{ \begin{array}{ll}
c \cdot \exp[-\frac{1}{2\sigma^2}(x-m)^2], & \textrm{where $x\leq m$}\\
c \cdot \exp[-\frac{1}{2\tau^2\sigma^2}(x-m)^2], & \textrm{where $x>m$}\\
\end{array} \right.
$$
where $c=\sqrt{\frac{2}{\pi}}\sigma^{-1}(1+\tau)^{-1}$. 

A natural generalization of the univariate split normal distribution to the multivariate settings was presented by \cite{villani2006multivariate}.
%\comment{(\cite{john1982three})}
Roughly speaking, authors assume that a vector $\x \in \R^d$ follows the multivariate Split Normal distribution, if its principal components are orthogonal and follow the one-dimensional Split Normal distribution.

\begin{definition}\label{def:SN}
A density of the multivariate Split Normal distribution is given by
$$
 SN_{d}(\x; \m, \sigma,\tau)= \prod_{j=1}^{d} SN(x_j;m_j,\sigma_j^2,\tau_j^2),
$$
where  $\m = [m_1, \ldots, m_d]^T$, $\sigma = [\sigma_{1}^2,\ldots,\sigma_{d}^2]^T$ and $\tau=[\tau_{1}^2,\ldots,\tau_{d}^2]^T$.
%where $W$ is the orthonormal matrix and $\w_{j}$ stand for the $j$-th column of $W$, $\m = (m_1, \ldots, m_d)^T$, $\sigma = (\sigma_{1},\ldots,\sigma_{d})$ and $\tau=(\tau_{1},\ldots,\tau_{d})$.
\end{definition}


In our case we will use density on projection on $d<D$ subspaces. Therefore we need a density $d$-subspace Split Normal distribution.

\begin{definition}\label{def:GSN}
A density of the multivariate $d$-subspace Split Normal distribution is given by
$$
 SN_{d<D}(\x; \m,W, \sigma^2,\tau^2)=  SN_d((W^TW)^{-1}W^T(\x-\m);0,\sigma^2,\tau^2),
$$
where
%%%%%%%%%
$(W^TW)^{-1}W^T(x-m) \in \R^d$
%%%%%%%%%
 $\w_{j} \in \R^D$ is the $j$-th column of non-singular matrix $W = [w_{1},\ldots,w_{d}]$, $\m = [m_1, \ldots, m_D]^T$, $\sigma = [\sigma_{1},\ldots,\sigma_{d}]^T$ and $\tau=[\tau_{1},\ldots,\tau_{d}]^T$.
\end{definition}

Let us recall that the standard Gaussian density in $\R^d$ is defined by 
$$
N(\x;\m,\Sigma)=\frac{1}{(2\pi)^{d/2} \det(\Sigma)^{1/2}} \exp \left(-\tfrac{1}{2} (\x-\m)^T \Sigma^{-1}(\x-\m) \right),
$$
where $\m$ denotes the mean, $\Sigma$ is the covariance matrix.

\begin{definition}\label{def:GSN}
A density of the multivariate $d$-subspace Normal distribution is given by
$$
 N_{d<D}(\x; \m, \Sigma, W)= N((W^TW)^{-1}W^T(\x-\m);0,\Sigma),
$$
where
%%%%%%%%%
$(W^TW)^{-1}W^T(\x-\m) \in \R^d$
%%%%%%%%%
 $\w_{j} \in \R^D$ is the $j$-th column of non-singular matrix $W = [w_{1},\ldots,w_{d}]$, $\m = [m_1, \ldots, m_D]^T$, $\Sigma = \diag(\sigma_{1}^2,\ldots,\sigma_{d}^2)$.
\end{definition}

Our goal is to minimize
$$
\KL(X,\F,\G)=\mle(X,\F)-\mle(X,\G) 
$$
In our language
\begin{equation}
\begin{array}{l}
\KL_{d<D}(X;\m,W,\sigma,\tau,\Sigma) = \\[6pt]
= \sum \limits_{\x \in X} \ln(SN_{d<D}(\x;\m,W,\sigma,\tau)) -
   \sum \limits_{\x \in X} \ln(N_{d<D}(\x;\m,\Sigma,W))
\end{array}
\end{equation}
We known
$$
\sum \limits_{\x \in X} \ln(N_{d<D}(\x;\m,\Sigma,W)) = -\frac{d}{2}\ln(2\pi e)-\frac{1}{2}\ln \det(\Sigma_{W}), 
$$
where 
$$
\Sigma_{W} = \cov( \{ (W^TW)^{-1}W^T(\x-\m) \colon \x \in \R^D\} )
$$

%%%%%%%%%%%%%%%%%%%
\subsection{Optimization problem}
%%%%%%%%%%%%%%%%%%%

The density of the multivariate d-subspace Normal distribution depends on four parameters $\m \in \R^d$, $W \in \M(\R^d)$, $\sigma \in \R^d$, $\tau \in \R^d$. 
We can find them by minimizing the simpler function, which depends on only  $m \in \R^d$ and $W \in \M(\R^d)$. Other parameters are given by explicit formulas. Let us notice that in this case our minimization problem simplifies to minimizing the function $\mle(X,\F) = \sum \limits_{\x \in X} \ln(SN_{d<D}(\x;\m,W,\sigma,\tau))$

\begin{theorem}\label{the:min}
Let $\x_1,\ldots,\x_n$ be given.  
Then the likelihood maximized w.r.t. $\sigma$ and $\tau$ is
\begin{equation}\label{eq:1}
%\min_{\sigma, \tau}
 \hat{L}(X;\m,W) =   \bigg( \frac{2n}{\pi e} \bigg)^{dn/2} \bigg( \prod_{j=1}^{d} g_{j}(\m,W) \bigg)^{-3n/2},
\end{equation}
where
$$
\begin{array}{c}
{g}_{j}(\m,W) = {s}_{1j}^{1/3} + {s}_{2j}^{1/3},
%\\[1ex]
%W_{\omega}=(W^TW)^{-1}W^T,
\\[1ex]
{s}_{1j}= \! \sum\limits_{i \in I_j}[ \w_{j}^T (\x_i-\m)]^2,  {I}_j=\{ i = 1,\ldots,n \colon \w_{j}^T (\x_i-\m) \leq 0 \},
\\[1ex]
{s}_{2j}= \! \sum\limits_{i \in I_j^c}[ \w_{j}^T (\x_i-\m)]^2, {I}_j^c=\{ i = 1,\ldots,n \colon  \w_{j}^T (\x_i-\m) > 0 \},
\end{array}
$$
where $\omega_j$ is the $j$-th column of non-singular matrix $(W^TW)^{-1}W^T$ and the maximum likelihood estimators of $\sigma_{j}^2$ and $\tau_{j}$ are
$$\hat \sigma_j^2(\m,W) = \tfrac{1}{n} s_{1j}^{2/3} g_{j}(\m,W), \quad
\hat \tau_{j}(\m,W)=\left(\frac{s_{2j}}{s_{1j}}\right)^{1/3}.
$$
\end{theorem}

\begin{proof}[Proof of Theorem \ref{the:min}.]
Let $X=\{ \x_1, \ldots, \x_n \}$ and $W_{\omega}=(W^TW)^{-1}W^T$.
We write 
$$
\z_i=  W_{\omega}(\x_i-m), \quad \z_{ij}= \omega_j^T(\x_i-m),
$$
for observation $i$, where $i=1,\ldots,n$ and coordinates $j=1,\ldots,d$.

Let us consider the likelihood function, i.e. 
$$
\begin{array}{l}
L(X;\m,W,\sigma,\tau) = \prod\limits_{i=1}^n SN_{d<D}(\x_i;\m,W,\sigma,\tau) =  
%= \sum \limits_{\x \in X} \ln(SN_{d<D}(\x;\m,W,\sigma,\tau)) 
\prod\limits_{i=1}^{n} \prod\limits_{j=1}^{d} SN(\omega_j^T(\x_i - \m) ; 0,\sigma^2,\tau^2)
\\[6pt]
= c_1^n \Big( \prod\limits_{j=1}^{d} \sigma_j(1+\tau_j) \Big)^{-n} %\cdot \\[6pt]
\prod\limits_{i=1}^{n} \prod\limits_{j=1}^{d} \exp \Big[ -\frac{1}{2\sigma_j^2}z_{ij}^2 (\1_{ \{ z_{ij} \leq 0 \} } + \tau_{j}^{-2} \1_{ \{ z_{ij} > 0 \} }) \Big],
\end{array}
$$
%$$
%\begin{array}{l}
%= \prod\limits_{i=1}^{n} GSN_d(\x_i ; \m,V,\sigma,\tau)
%= \prod\limits_{i=1}^{n} \frac{1}{| \det( V)|}  \prod\limits_{j=1}^{d} SN(  \v^{-1}_j \x_i ; \v^{-1}_j \m , \sigma_j^2, \tau_j^2)=
%\\[1ex]
%\left(\frac{c_1}{|\det(V)|}\right)^{n} \left( \prod\limits_{j=1}^{d} \sigma_j(1+\tau_j) \right)^{-n} \left( \prod\limits_{i=1}^{n} \prod\limits_{j=1}^{d} \exp[-\frac{1}{2\sigma_j^2}z_{ij}^2 (\1_{ \{ z_{ij} \leq 0 \} } + \tau_{j}^{-2} \1_{ \{ z_{ij} > 0 \} })]\right)
%\end{array}
%$$
where 
$
c_1=\left( \sqrt{\tfrac{2}{\pi}} \right)^{d}.
$
Now we take the log-likelihood function, i.e.
$$
\begin{array}{l}
\ln(L(X;\m,W,\sigma,\tau)) \\[6pt]
=\ln \bigg( c_1^n \Big( \prod\limits_{j=1}^{d} \sigma_j(1+\tau_j) \Big)^{-n} \bigg) + %\\[6pt]
 \sum\limits_{i=1}^{n} \sum\limits_{j=1}^{d} \Big[ -\frac{1}{2\sigma_j^2}z_{ij}^2 (\1_{ \{ z_{ij} \leq 0 \} } + \tau_{j}^{-2} \1_{ \{ z_{ij} > 0 \} })\Big]  \\[6pt]
= \ln \bigg( c_1^n \Big( \prod\limits_{j=1}^{d} \sigma_j(1+\tau_j) \Big)^{-n} \bigg)  -%\\[6pt]
  \frac{1}{2} \sum\limits_{j=1}^{d} \Big( \sigma_j^{-2} \sum\limits_{i \in I_{j}}    z_{ij}^2   + \frac{\sigma_j^{-2}}{\tau_{j}^{2} }  \sum\limits_{i \in I_{j}^{c}}   z_{ij}^2  \Big) \\[6pt]
= \ln \bigg( c_1^n \Big( \prod\limits_{j=1}^{d} \sigma_j(1+\tau_j) \Big)^{-n} \bigg)  - 
 \sum\limits_{j=1}^{d} \frac{1}{2\sigma_j^{2}} \Big(  s_{1j}  + \frac{1}{\tau_{j}^{2} }  s_{2j}  \Big).
\end{array}
$$

We fix  $\m$, $W$ and maximize the log-likelihood function over $\tau$ and $\sigma$.
In such a case we have to solve the following system of equations
$$
\begin{array}{l}
\frac{\partial  \ln ( L(X;\m,W,\sigma,\tau) ) }{\partial \sigma_j} = -\frac{n}{\sigma_j} +  \sigma_j^{-3} (s_{1j} + \tau_j^{-2} s_{2j} )
 =0, \\[6pt] %& \mbox{ for }  & j=1,\ldots,d,
 \frac{\partial  \ln ( L(X;\m,W,\sigma,\tau) ) }{\partial \tau_j} = - \frac{n}{1+\tau_j} + \frac{s_{2j}}{\tau_j^{3}\sigma_j^{2}} =0 , %& \mbox{ for }  & j=1,\ldots,d.
\end{array}
$$
for  $ j=1,\ldots,d$.
By simple calculations we obtain the expressions for the estimators
%\begin{align*}
$$
\hat{\sigma}_j^2(\m,W) = 
\tfrac{1}{n} s_{1j}^{2/3} g_{j}(\m,W), \qquad
\hat{\tau}_{j}(\m,W) = \bigg( \frac{s_{2j}}{s_{1j}} \bigg)^{1/3}.
$$
%\end{align*}
Substituting it into the log-likelihood function,
%and taking $e^{\ln \hat{L}(\m,W)}$
we get
$$
\begin{array}{l}
\hat{L}(\m,W) = \bigg( \frac{2}{\pi} \bigg)^{\frac{dn}{2}} \Big( \prod\limits_{j=1}^{d} \frac{1}{\sqrt{n}} g_j(\m,W)^{\frac{3}{2}} \Big)^{-n}  e^{-\frac{dn}{2}}\\[6pt]
= \bigg( \frac{2n}{\pi e} \bigg)^{\frac{dn}{2}}  \Big( \prod\limits_{j=1}^{d} g_j(\m,W) \Big)^{-\frac{3n}{2}}. 
\end{array}
$$
\end{proof}

%\begin{theorem}\label{the:Villani}
%Given a random sample $\x_1,\ldots,\x_n$ from $SN(\mu,\lambda^2,\tau^2)$, the likelihood, maximized over $\lambda$ and $\tau$, is
%\begin{equation}\label{eq:Villani}
%%\min_{\sigma, \tau}
% \hat{L}(\mu) =   \bigg( \frac{2n}{\pi e} \bigg)^{n/2} \bigg( \prod_{j=1}^{d} g_{j}(\m,W) \bigg)^{-3n/2},
%\end{equation}
%where
%$$
%\begin{array}{c}
%{g}_{j}(\m,W) = {s}_{1j}^{1/3} + {s}_{2j}^{1/3},
%\\[1ex]
%{s}_{1j}= \! \sum\limits_{i \in I_j}[ \w_{j}^T (\x_i-\m)]^2,  {I}_j=\{ i = 1,\ldots,n \colon \w_{j}^T (\x_i-\m) \leq 0 \},
%\\[1ex]
%{s}_{2j}= \! \sum\limits_{i \in I_j^c}[ \w_{j}^T (\x_i-\m)]^2, {I}_j^c=\{ i = 1,\ldots,n \colon  \w_{j}^T (\x_i-\m) > 0 \},
%\end{array}
%$$
%and the maximum likelihood estimators of $\sigma_{j}^2$ and $\tau_{j}$ are
%$$\hat \sigma_j^2(\m,W) = \tfrac{1}{n} s_{1j}^{2/3} g_{j}(\m,W), \quad
%\hat \tau_{j}(\m,W)=\left(\frac{s_{2j}}{s_{1j}}\right)^{1/3}.
%$$
%\end{theorem}


Thanks to the above theorem, instead of looking for the maximum of the likelihood function, it is enough to obtain the maximum of the simpler function~(\ref{eq:1}) which depends on two parameters $\m \in \R^d$ and $W \in \M(\R^d)$
\begin{equation}\label{equ:ll}
{l}(X;\m,W) = \prod_{j=1}^{d} {g}_{j}(\m,W)
\end{equation}
where $\w_{j}$ stands for the $j$-th column of matrix $W$. 
Consequently, maximization of (\ref{eq:1}) is equivalent to minimization of  (\ref{equ:ll}), see the following corollary.

\begin{corollary}\label{c2}
Let $X \subset \R^d$, $\m \in \R^d$, $W \in \M(\R^d)$ be given, then
$$
 \text{argmax}_{\m,W} \hat{L}(X;\m,W) =  \argmin_{\m,W} {l}(X;\m,W).
$$
\end{corollary}
%\begin{proof}
%Notice that $l(\m,V,X) = l(\m,W^{-1},X)= {l}(X;\m,W)$ where $W=V^{-1}$ and $$
%\argmax_{\m,V} \hat L(\m,V,X) = \argmin_{\m,V} l(\m,V,X).
%$$
%Moreover
%$$
%\min_{\m,V} l(\m,V,X) = \min_{\m,W} {l}(X;\m,W).
%$$
%
%\end{proof} 


%%%%%%%%%%%%%%%%%%%
\subsection{Gradient}
%%%%%%%%%%%%%%%%%%%

%Minimization of 

%In this subsection we calculate the gradient of the function $\ln({l})$.

One of the possible methods of optimization is the gradient method. Since the minimum of ${l}$ is equal to the minimum of $\ln({l})$, in this subsection we calculate the gradient of $\ln({l})$. 
Before we prove suitable Theorem \ref{ther:grad}, we recall the following lemma. 

\begin{lemma}\label{jacobi}
%\comment{Jacek mowi, ze powino być A(t), i napisać co to znaczy adj()}
Let $A = (a_{ij})_{1 \leq i,j \leq d}$ be a differentiable map from real numbers to $d \times d$ matrices then
\begin{equation}
\frac{\partial \det(A)}{\partial a_{ij}} = \mathrm{adj}^T(A)_{ij},
\end{equation}
where $\mathrm{adj}(A)$ stands for the adjugate of $A$, i.e. the transpose of the cofactor matrix.
\end{lemma}
\begin{proof}
By the Laplace expansion $\det A = \sum\limits_{j=1}^{d} (-1)^{i+j} a_{ij} M_{ij}$ where $M_{ij}$ is the minor of the entry in the $i$-th row and $j$-th column. Hence
$$\frac{\partial \det A}{\partial a_{ij}} = (-1)^{i+j} M_{ij} = \mathrm{adj}^T(A)_{ij}.$$
\end{proof}
Now we are ready to calculate gradient of our cost function.

\begin{theorem}\label{ther:grad}
Let $X \subset \R^d$, $\m = (\m_1, \ldots, \m_d)^T \in \R^d$, $W = (\w_{ij})_{1 \leq i,j \leq d}$ non-singular be given. 
Then
$\nabla_{\m}  \ln {l}(X;\m,W) = \left(  \frac{\partial \ln {l}(X;\m,W)}{\partial \m_1}, \ldots, \frac{\partial \ln {l}(X;\m,W)}{\partial \m_d} \right)^T$,
where
$$
\begin{array}{l}
\frac{\partial \ln {l}(X;\m,W)}{\partial \m_k} =
\sum \limits_{j=1}^d \frac{-1}{{s}_{1j}^{\frac{1}{3}} + {s}_{2j}^{\frac{1}{3}}} \bigg(
\frac{1}{3 {s}_{1j}^{\frac{2}{3}}} \sum \limits_{i \in I_j} 2 \w_j^T (\x_i - \m)  \w_{jk} + %\\[6pt]
\frac{1}{3 {s}_{2j}^{\frac{2}{3}}} \sum \limits_{i \in I_j^c} 2 \w_j^T (\x_i - \m)  \w_{jk}
\bigg).
\end{array}
$$
Moreover,
$
\nabla_{W} \ln {l}(X;\m,W) = \left[ \frac{\partial \ln \tilde{l}(X;\m,W)}{\partial \w_{pk}}  \right]_{1 \leq p,k \leq d},
$
where
$$
\begin{array}{l}
\frac{\partial \ln \tilde{l}(X;\m,W)}{\partial \w_{pk}}  = 
-\frac{2}{3}  (\w^{-1})^T_{pk} +%\\[6pt] 
\frac{1}{{s}_{1p}^{\frac{1}{3}} +{s}_{2p}^{\frac{1}{3}}} 
\bigg(
\frac{1}{3} {s}_{1p}^{-\frac{2}{3}}  \sum \limits_{i \in {I}_p} 2 \w^T_p  (\x_i - \m) (\x_{ik} - \m_k) + \\[6pt]
+ \frac{1}{3} {s}_{2p}^{-\frac{2}{3}}  \sum \limits_{i \in {I}_p^c} 2 \w^T_p  (\x_i - \m) (\x_{ik} - \m_k) \bigg).
\end{array}
$$
and
$$
\begin{array}{c}
%{g}_{j}(\m,W) = {s}_{1j}^{1/3} + {s}_{2j}^{1/3},
%\\[1ex]
{s}_{1j}= \! \sum\limits_{i \in I_j}[ \w_{j}^T (\x_i-\m)]^2, {I}_j=\{ i = 1,\ldots,n \colon \w_{j}^T (\x_i-\m) \leq 0 \},
\\[1ex]
{s}_{2j}= \! \sum\limits_{i \in I_j^c}[ \w_{j}^T (\x_i-\m)]^2,  {I}_j^c=\{ i = 1,\ldots,n \colon  \w_{j}^T (\x_i-\m) > 0 \}.
\end{array}
$$
\end{theorem}

\begin{proof}[Proof of Theorem \ref{ther:grad}.]
Let us start with the partial derivative of $\ln({l})$ with respect to $\m$. We have
$$
\begin{array}{l}
\frac{\partial \ln {l}(X;\m,W)}{\partial \m_k} =
\sum \limits_{j=1}^d \frac{\partial \ln ({g}_j(\m,W))}{\partial \m_k} = \sum\limits_{j=1}^d \frac{1}{{s}_{1j}^{\frac{1}{3}} + {s}_{2j}^{\frac{1}{3}}} \frac{\partial ({s}_{1j}^{\frac{1}{3}} + {s}_{2j}^{\frac{1}{3}})}{\partial \m_k} %=\\[6pt]
 \sum \limits_{j=1}^d \frac{1}{{s}_{1j}^{\frac{1}{3}} + {s}_{2j}^{\frac{1}{3}}} \bigg(
\frac{1}{3 {s}_{1j}^{\frac{2}{3}}} \frac{\partial {s}_{1j}}{\partial \m_k} +
\frac{1}{3 {s}_{2j}^{\frac{2}{3}}} \frac{\partial {s}_{2j}}{\partial \m_k}
\bigg).
\end{array}
$$
Now, we need $\frac{\partial {s}_{1j}}{\partial \m_k}$ and $\frac{\partial {s}_{2j}}{\partial \m_k}$, therefore
$$
\begin{array}{l}
\frac{\partial {s}_{1j}}{\partial \m_k} = 
\sum\limits_{i \in {I}_j} \frac{\partial [\w^T_j (\x_i - \m)]^2}{\partial \m_k} = \sum\limits_{i \in {I}_j} 2 \w^T_j (\x_i - \m) \frac{\partial \w^T_j (\x_i - \m)}{\partial \m_k} = %\\[6pt]
 \sum\limits_{i \in {I}_j} - 2 \w^T_j (\x_i - \m) \w_{jk}.
\end{array}
$$
Analogously we get
$$
\begin{array}{l}
\frac{\partial {s}_{2j}}{\partial \m_k} = \sum\limits_{i \in {I}_j^c} -2 \w^T_j (\x_i - \m) \w_{jk}.
\end{array}
$$
%\comment{$\v^{-1}_{jk} = \w_{jk}$}\\
Hence 
$$
\begin{array}{l}
\frac{\partial \ln {l}}{\partial \m_k} =\sum\limits_{j=1}^d \frac{-1}{{s}_{1j}^{\frac{1}{3}} + {s}_{2j}^{\frac{1}{3}}} \bigg(
\frac{1}{3 {s}_{1j}^{\frac{2}{3}}} \sum\limits_{i \in I_j} 2 \w_j^T (\x_i - \m)  \w_{jk} +% \\[6pt]
\frac{1}{3 {s}_{2j}^{\frac{2}{3}}} \sum\limits_{i \in I_j^c} 2 \w_j^T (\x_i - \m) \w_{jk}
\bigg).
\end{array}
$$

Now we calculate the partial derivative of $\ln {l}(X;\m,W)$ with respect to the matrix $W$. We have
$$
\begin{array}{l}
\frac{\partial \ln {l}(X;\m,W)}{\partial \w_{pk}} = \frac{\partial \ln |\det(W)|^{-\frac{2}{3}}}{\partial \w_{pk}} + \sum\limits_{j=1}^d \frac{\partial \ln ({g}_j(\m,W))}{\partial \w_{pk}}.
\end{array}
$$
%\comment{$\v_{pk}^{-1} = \w_{pk}$}\\
To calculate the derivative of the determinant we use Jacobi's formula (see Lemma \ref{jacobi}).
Hence% $\frac{\partial \ln (\det(W)^{-\frac{2}{3}})}{\partial \w_{pk}} =$
$$
\begin{array}{l}
\frac{\partial \ln (\det(W)^{-\frac{2}{3}})}{\partial \w_{pk}} = \det(W)^{\frac{2}{3}}  \Big(-\frac{2}{3}\Big)  \det(W)^{-\frac{5}{3}}  \frac{\partial \det(W)}{\partial \w_{pk}} = -\frac{2}{3} \det(W)^{-1}  \mathrm{adj}^T(W)_{pk} \\[6pt]
 = -\frac{2}{3} \frac{1}{\det(W)}  \left[\det(W)  (W^{-1})^T_{pk}\right]= -\frac{2}{3}  (\w^{-1})^T_{pk},
\end{array}
$$
where $(\w^{-1})^T_{pk}$ is the element in the $p$-th row and $k$-th column of the matrix $(W^{-1})^T$. Now we calculate %$\frac{\partial \ln ({g}_j(\m,W))}{\partial \w_{pk}} =$
$$
\begin{array}{l}
\frac{\partial \ln ({g}_j(\m,W))}{\partial \w_{pk}} = \frac{1}{{s}_{1j}^{\frac{1}{3}} + {s}_{2j}^{\frac{1}{3}}} \frac{\partial ({s}_{1j}^{\frac{1}{3}} + {s}_{2j}^{\frac{1}{3}})}{\partial \w_{pk}}= \frac{1}{{s}_{1j}^{\frac{1}{3}} + {s}_{2j}^{\frac{1}{3}}} \bigg(
\frac{1}{3 {s}_{1j}^{\frac{2}{3}}}  \frac{\partial {s}_{1j}}{\partial \w_{pk}} +
\frac{1}{3 {s}_{2j}^{\frac{2}{3}}}  \frac{\partial {s}_{2j}}{\partial \w_{pk}}
\bigg),
\end{array}
$$
where
$$
\begin{array}{l}
\frac{\partial {s}_{1j}}{\partial \w_{pk}} = \sum\limits_{ i \in {I}_j} \frac{\partial [\w^T_j (\x_i - \m)]^2}{\partial \w_{pk}} = \sum\limits_{ i \in {I}_j} 2 \w^T_j (\x_i - \m) \frac{\partial \w^T_j (\x_i - \m)}{\partial \w_{pk}}=
\\[6pt]
\left\{ \begin{array}{ll}
0, & \text{if} \; j\neq p\\
\sum\limits_{ i \in {I}_p} 2 \w^T_p (\x_i - \m) (\x_{ik} - \m_k), & \text{if} \; j=p\\
\end{array} \right.
\end{array}
$$
and $\x_{ik}$ is the $k$-th element of the vector $\x_i$. Analogously we get
$$\frac{\partial {s}_{2j}}{\partial \w_{pk}} = \left\{ \begin{array}{ll}
0, & \text{if} \; j\neq p\\
\sum\limits_{ i \in {I}_p^c} 2 \w^T_p (\x_i - \m) (\x_{ik} - \m_k), & \text{if} \; j=p.
\end{array} \right.
$$
Hence we obtain %$\frac{\partial \ln {l}}{\partial \w_{pk}} =$
$$
\begin{array}{l}
\frac{\partial \ln {l}}{\partial \w_{pk}} = -\frac{2}{3} (\w^{-1})^T_{pk} + \frac{1}{{s}_{1p}^{\frac{1}{3}} +{s}_{2p}^{\frac{1}{3}}} 
 \bigg(
\frac{1}{3} {s}_{1p}^{-\frac{2}{3}} \sum\limits_{ i \in {I}_p} 2 \w^T_p (\x_i - \m) (\x_{ik} - \m_k)\\[6pt]
+ \frac{1}{3} {s}_{2p}^{-\frac{2}{3}} \sum\limits_{ i \in {I}_p^c} 2 \w^T_p (\x_i - \m) (\x_{ik} - \m_k) \bigg).
\end{array}
$$
\end{proof}



\section{MODEL II}

\begin{definition}\label{def:GSN}
A density of the multivariate Split Normal $d$ and Normal $D-d$ distribution is given by
$$
 SN_{d}N_{D-d}(\x; \m,W, \sigma^2,\tau^2)=\det(W) \prod_{j=1}^{d} SN(\w_j^T(\x-\m);0,\sigma_j^2,\tau_j^2)\prod_{j=d+1}^{D} N(\w_j^T(\x-\m);0,\sigma_j^2),
$$
where $\w_{j}$ is the $j$-th column of non-singular matrix $W$, $\m = (m_1, \ldots, m_d)^T$, $\sigma = (\sigma_{1},\ldots,\sigma_{d})$ and $\tau=(\tau_{1},\ldots,\tau_{D-d})$.
\end{definition}
